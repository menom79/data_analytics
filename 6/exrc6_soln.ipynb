{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c49421c-e0ec-477b-b392-7502283c5a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9891696750902527\n",
      "Confusion Matrix:\n",
      "[[ 465   12]\n",
      " [   9 1453]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the wine data\n",
    "wine_data = pd.read_csv('exrc06p01_wine.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "wine_data.dropna(inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = wine_data.drop(columns=['type'])\n",
    "y = wine_data['type']\n",
    "\n",
    "# Split the data into train and test sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ed91f7-f95f-42a9-95aa-84f9ef36515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6624605678233438\n",
      "Confusion Matrix:\n",
      "[[261 191]\n",
      " [130 369]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the voice data\n",
    "voice_data = pd.read_csv('exrc06p02_voice.csv')\n",
    "\n",
    "# Encode the label column\n",
    "label_encoder = LabelEncoder()\n",
    "voice_data['label'] = label_encoder.fit_transform(voice_data['label'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = voice_data.drop(columns=['label'])\n",
    "y = voice_data['label']\n",
    "\n",
    "# Split the data into train and test sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Support Vector Machine model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87c2bef-4eca-4f08-9209-0fe0b9a9e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.7213930348258707\n",
      "Confusion Matrix:\n",
      "[[ 78  78]\n",
      " [ 34 212]]\n",
      "\n",
      "Support Vector Machine:\n",
      "Accuracy: 0.7189054726368159\n",
      "Confusion Matrix:\n",
      "[[ 80  76]\n",
      " [ 37 209]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the NBA dataset\n",
    "nba_data = pd.read_csv('exrc06p03_nba.csv')\n",
    "\n",
    "# Fill missing values with the median of each column\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "nba_data_filled = pd.DataFrame(imputer.fit_transform(nba_data.select_dtypes(include=['number'])), columns=nba_data.select_dtypes(include=['number']).columns)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "nba_data_filled['TARGET_5Yrs'] = label_encoder.fit_transform(nba_data['TARGET_5Yrs'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = nba_data_filled.drop(columns=['TARGET_5Yrs'])\n",
    "y = nba_data_filled['TARGET_5Yrs']\n",
    "\n",
    "# Split the dataset into train and test sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "logistic_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Logistic Regression model\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_pred)\n",
    "logistic_cm = confusion_matrix(y_test, logistic_pred)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", logistic_accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(logistic_cm)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate SVM model\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_cm = confusion_matrix(y_test, svm_pred)\n",
    "\n",
    "print(\"\\nSupport Vector Machine:\")\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(svm_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbaea94-11d8-478d-8608-721dd5ae336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[1257    0]\n",
      " [   0 1181]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the mushroom dataset\n",
    "mushroom_data = pd.read_csv('exrc06p04_mushrooms.csv')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = mushroom_data.drop(columns=['class'])\n",
    "y = mushroom_data['class']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Split the dataset into train and test sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9c6421-8d48-42fe-9904-487dea992d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 846 entries, 0 to 980\n",
      "Data columns (total 15 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Gender                   846 non-null    int64  \n",
      " 1   Married                  846 non-null    int64  \n",
      " 2   Education                846 non-null    int64  \n",
      " 3   Self_Employed            846 non-null    int64  \n",
      " 4   ApplicantIncome          846 non-null    int64  \n",
      " 5   CoapplicantIncome        846 non-null    float64\n",
      " 6   LoanAmount               846 non-null    float64\n",
      " 7   Loan_Amount_Term         846 non-null    float64\n",
      " 8   Credit_History           846 non-null    float64\n",
      " 9   Loan_Status              846 non-null    int64  \n",
      " 10  Dependents_1.0           846 non-null    uint8  \n",
      " 11  Dependents_2.0           846 non-null    uint8  \n",
      " 12  Dependents_3.0           846 non-null    uint8  \n",
      " 13  Property_Area_Semiurban  846 non-null    uint8  \n",
      " 14  Property_Area_Urban      846 non-null    uint8  \n",
      "dtypes: float64(4), int64(6), uint8(5)\n",
      "memory usage: 76.8 KB\n",
      "Probability: 86.98%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Load the loan dataset\n",
    "loan_data_path = 'loan.csv'\n",
    "loan_data = pd.read_csv(loan_data_path)\n",
    "\n",
    "# Let's view the first few rows of the dataframe and summarize missing values\n",
    "loan_data.head()\n",
    "\n",
    "loan_data.isnull().sum()\n",
    "\n",
    "# Drop the Loan_ID column as it is not relevant for prediction\n",
    "loan_data.drop('Loan_ID', axis=1, inplace=True)\n",
    "\n",
    "# Encode binary categorical fields to 0/1\n",
    "binary_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Loan_Status']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in binary_cols:\n",
    "    # Drop rows where binary categorical column is NaN before encoding\n",
    "    loan_data = loan_data.dropna(subset=[col])\n",
    "    loan_data[col] = label_encoder.fit_transform(loan_data[col])\n",
    "\n",
    "# Encode Dependents as it has a small number of unique values\n",
    "# First, fill missing values with the most frequent value\n",
    "most_frequent_dependent = loan_data['Dependents'].mode()[0]\n",
    "loan_data['Dependents'].fillna(most_frequent_dependent, inplace=True)\n",
    "loan_data = pd.get_dummies(loan_data, columns=['Dependents', 'Property_Area'], drop_first=True)\n",
    "\n",
    "# Replace missing values in numerical columns with the median\n",
    "numerical_cols = ['LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n",
    "loan_data[numerical_cols] = loan_data[numerical_cols].fillna(loan_data[numerical_cols].median())\n",
    "\n",
    "# Remove outliers more than 3 standard deviations from the mean\n",
    "numerical_cols.extend(['ApplicantIncome', 'CoapplicantIncome'])\n",
    "for col in numerical_cols:\n",
    "    mean = loan_data[col].mean()\n",
    "    std_dev = loan_data[col].std()\n",
    "    loan_data = loan_data[(loan_data[col] <= mean + 3 * std_dev) & (loan_data[col] >= mean - 3 * std_dev)]\n",
    "\n",
    "# Check the modified dataframe\n",
    "loan_data.head()\n",
    "\n",
    "loan_data.info()\n",
    "\n",
    "# Separate the target variable and features\n",
    "y_loan = loan_data['Loan_Status']\n",
    "X_loan = loan_data.drop('Loan_Status', axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_loan, X_test_loan, y_train_loan, y_test_loan = train_test_split(X_loan, y_loan, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg_loan = LogisticRegression(max_iter=1000)\n",
    "log_reg_loan.fit(X_train_loan, y_train_loan)\n",
    "\n",
    "# Given data point for prediction\n",
    "data_point = pd.DataFrame({\n",
    "    'Gender': [1],  # Male\n",
    "    'Married': [0],  # No\n",
    "    'Education': [0],  # Graduate\n",
    "    'Self_Employed': [0],  # No\n",
    "    'ApplicantIncome': [2400],\n",
    "    'CoapplicantIncome': [2000],\n",
    "    'LoanAmount': [36],\n",
    "    'Loan_Amount_Term': [360],\n",
    "    'Credit_History': [1],  # 1\n",
    "    'Dependents_1.0': [0],\n",
    "    'Dependents_2.0': [0],\n",
    "    'Dependents_3.0': [0],\n",
    "    'Property_Area_Semiurban': [0],\n",
    "    'Property_Area_Urban': [1]  # Urban\n",
    "})\n",
    "\n",
    "# Predict the probability of Loan_Status being Yes\n",
    "probability = log_reg_loan.predict_proba(data_point)[:, 1]\n",
    "\n",
    "print(f\"Probability: {probability[0]*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd76b51-8a8a-45a4-a1fd-dcb1162edad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Data Point for Prediction:\n",
      "   Gender  Married  Education  Self_Employed  ApplicantIncome  \\\n",
      "0       1        0          0              0             2400   \n",
      "\n",
      "   CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n",
      "0               2000          36               360               1   \n",
      "\n",
      "   Dependents_1.0  Dependents_2.0  Dependents_3.0  Property_Area_Semiurban  \\\n",
      "0               0               0               0                        0   \n",
      "\n",
      "   Property_Area_Urban  \n",
      "0                    1  \n",
      "\n",
      "Probability of Loan_Status being Yes:\n",
      "85.74%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the loan dataset\n",
    "loan_data_path = 'loan.csv'\n",
    "loan_data = pd.read_csv(loan_data_path)\n",
    "\n",
    "# Drop the Loan_ID column as it is not relevant for prediction\n",
    "loan_data.drop('Loan_ID', axis=1, inplace=True)\n",
    "\n",
    "# Encode binary categorical fields to 0/1\n",
    "binary_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Loan_Status']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in binary_cols:\n",
    "    loan_data[col] = label_encoder.fit_transform(loan_data[col])\n",
    "\n",
    "# Encode Dependents as it has a small number of unique values\n",
    "# First, fill missing values with the most frequent value\n",
    "most_frequent_dependent = loan_data['Dependents'].mode()[0]\n",
    "loan_data['Dependents'].fillna(most_frequent_dependent, inplace=True)\n",
    "loan_data = pd.get_dummies(loan_data, columns=['Dependents', 'Property_Area'], drop_first=True)\n",
    "\n",
    "# Replace missing values in numerical columns with the median\n",
    "numerical_cols = ['LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n",
    "loan_data[numerical_cols] = loan_data[numerical_cols].fillna(loan_data[numerical_cols].median())\n",
    "\n",
    "# Remove outliers more than 3 standard deviations from the mean\n",
    "numerical_cols.extend(['ApplicantIncome', 'CoapplicantIncome'])\n",
    "for col in numerical_cols:\n",
    "    mean = loan_data[col].mean()\n",
    "    std_dev = loan_data[col].std()\n",
    "    loan_data = loan_data[(loan_data[col] <= mean + 3 * std_dev) & (loan_data[col] >= mean - 3 * std_dev)]\n",
    "\n",
    "# Separate the target variable and features\n",
    "y_loan = loan_data['Loan_Status']\n",
    "X_loan = loan_data.drop('Loan_Status', axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_loan, X_test_loan, y_train_loan, y_test_loan = train_test_split(X_loan, y_loan, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg_loan = LogisticRegression(max_iter=1000)\n",
    "log_reg_loan.fit(X_train_loan, y_train_loan)\n",
    "\n",
    "# Given data point for prediction\n",
    "data_point = pd.DataFrame({\n",
    "    'Gender': [1],  # Male\n",
    "    'Married': [0],  # No\n",
    "    'Education': [0],  # Graduate\n",
    "    'Self_Employed': [0],  # No\n",
    "    'ApplicantIncome': [2400],\n",
    "    'CoapplicantIncome': [2000],\n",
    "    'LoanAmount': [36],\n",
    "    'Loan_Amount_Term': [360],\n",
    "    'Credit_History': [1],  # 1\n",
    "    'Dependents_1.0': [0],\n",
    "    'Dependents_2.0': [0],\n",
    "    'Dependents_3.0': [0],\n",
    "    'Property_Area_Semiurban': [0],\n",
    "    'Property_Area_Urban': [1]  # Urban\n",
    "})\n",
    "\n",
    "# Predict the probability of Loan_Status being Yes\n",
    "probability = log_reg_loan.predict_proba(data_point)[:, 1]\n",
    "\n",
    "# print(f\"Probability: {probability[0]*100:.2f}%\")\n",
    "\n",
    "print(\"Given Data Point for Prediction:\")\n",
    "print(data_point)\n",
    "print(\"\\nProbability of Loan_Status being Yes:\")\n",
    "print(f\"{probability[0]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1ac2ca0-f188-41ad-b275-5638f40dfb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the relevant .ipynb files in your current directory:\n",
      "\n",
      "  1. exrc6_soln.ipynb\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input the order number (the one in the beginning of the line) of the file you want to hand in:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your answers to exercises 06 were handed in successfully. Thank you!\n",
      "You may double check your handin by calling\n",
      "\n",
      "    /home/varpha/data_analytics/bin/check_handin.sh\n",
      "\n",
      "from the terminal prompt.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/varpha/data_analytics/lib')\n",
    "from handin import handin_exrc_06\n",
    "handin_exrc_06()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf61c4-86e1-49c7-8095-f7c94e27861f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
